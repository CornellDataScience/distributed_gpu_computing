{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serverteam_1/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss_and_accuracy(images, labels):\n",
    "  \"\"\" \n",
    "  Constructs the model training graph, which reads batches of data from data_dir and outputs the model's loss/zero-one accuracy on that batch.  \n",
    "  Data is read only from files in data_dir with the specified filename prefix.\n",
    "  \n",
    "  Returns a tuple of tensors (loss, accuracy) corresponding to our model's loss/zero-one accuracy on a batch of data loaded from data_dir.\n",
    "  \"\"\"\n",
    "  # Import the mnist module here so that it's available on the Spark executors within which this function will be run.\n",
    "  from tensorflow.examples.tutorials.mnist import mnist  \n",
    "  \n",
    "  # Taken from https://github.com/tensorflow/tensorflow/blob/v1.4.0/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py#L131\n",
    "  # Build a Graph that computes predictions from the inference model.\n",
    "  logits = mnist.inference(images, hidden1_units=100, hidden2_units=100)\n",
    "\n",
    "  # Add to the Graph the loss calculation, as well as an op for computing the zero-one accuracy of the model\n",
    "  batch_size = tf.shape(images)[0]\n",
    "  evaluation = mnist.evaluation(logits, labels)\n",
    "  accuracy = tf.cast(evaluation, tf.float32) / tf.cast(batch_size, tf.float32)\n",
    "  return (mnist.loss(logits, labels), accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_optimizers_and_launch_sess(loss, learning_rate, checkpoint_dir, task_index, num_workers, global_step, server):\n",
    "  # Determine if current process is the chief worker\n",
    "  is_chief = (task_index == 0)\n",
    "  \n",
    "  # Create Adam optimizer and wrap it in a SyncReplicasOptimizer, which coordinates updates across workers\n",
    "  # For more information see https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/train/SyncReplicasOptimizer\n",
    "  opt = tf.train.AdamOptimizer(LEARNING_RATE)        \n",
    "  opt = tf.train.SyncReplicasOptimizer(\n",
    "      opt,\n",
    "      replicas_to_aggregate=num_workers,\n",
    "      total_num_replicas=num_workers,\n",
    "      name=\"mnist_sync_replicas\")\n",
    "  # Compute gradients with respect to the loss.\n",
    "  grads = opt.compute_gradients(loss)\n",
    "  apply_gradients_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "  with tf.control_dependencies([apply_gradients_op]):\n",
    "    train_op = tf.identity(loss, name='train_op')\n",
    "\n",
    "  init_op = tf.global_variables_initializer()\n",
    "  # Get additional ops that must be run to initialize the SyncReplicasOptimizer\n",
    "  chief_queue_runner = opt.get_chief_queue_runner()\n",
    "  init_tokens_op = opt.get_init_tokens_op()\n",
    "\n",
    "  # Create a Supervisor to manage model checkpointing\n",
    "  # See https://www.tensorflow.org/api_docs/python/tf/train/Supervisor for more info\n",
    "  sv = tf.train.Supervisor(\n",
    "      is_chief=is_chief,\n",
    "      logdir=CHECKPOINT_DIR,\n",
    "      init_op=init_op,\n",
    "      global_step=global_step,\n",
    "      save_model_secs=30,\n",
    "      recovery_wait_secs=1\n",
    "  )\n",
    "\n",
    "  # Create session config\n",
    "  sess_config = tf.ConfigProto(\n",
    "      allow_soft_placement=True,\n",
    "      log_device_placement=False,\n",
    "      device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % task_index])\n",
    "  \n",
    "  # The chief worker (task_index==0) will prepare the session,\n",
    "  # while the remaining workers will wait for the session to be available.\n",
    "  sess = sv.prepare_or_wait_for_session(server.target, config=sess_config)\n",
    "  \n",
    "  if is_chief:\n",
    "    # Chief worker will start the chief queue runner and call the init op.\n",
    "    sv.start_queue_runners(sess, [chief_queue_runner])        \n",
    "    sess.run(init_tokens_op)  \n",
    "    \n",
    "  return (sess, train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
