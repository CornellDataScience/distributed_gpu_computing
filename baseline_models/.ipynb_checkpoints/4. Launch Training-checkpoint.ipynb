{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:File `'./Constants.py'` not found.\n"
     ]
    }
   ],
   "source": [
    "%run \"./Constants\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:File `'./Data Ingest.py'` not found.\n"
     ]
    }
   ],
   "source": [
    "%run \"./Data Ingest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:File `'./Constructing the Model Graph.py'` not found.\n"
     ]
    }
   ],
   "source": [
    "%run \"./Constructing the Model Graph\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'REMOVE_OLD_DIRS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d44098bd396b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mREMOVE_OLD_DIRS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;31m# Remove old checkpoint directory, old event file directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mcheckpoint_dir_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCHECKPOINT_DIR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/dbfs/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mdbutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mevent_file_dir_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDBFS_EVENT_FILE_DIR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/dbfs/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'REMOVE_OLD_DIRS' is not defined"
     ]
    }
   ],
   "source": [
    "if REMOVE_OLD_DIRS:\n",
    "  # Remove old checkpoint directory, old event file directory\n",
    "  checkpoint_dir_name = CHECKPOINT_DIR.split(\"/dbfs/\")[1]\n",
    "  dbutils.fs.rm(checkpoint_dir_name, recurse=True)\n",
    "  event_file_dir_name = DBFS_EVENT_FILE_DIR.split(\"/dbfs/\")[1]\n",
    "  dbutils.fs.rm(event_file_dir_name, recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'SyncUtils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-83a4244bb85c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# First, add a Python module containing utilities for syncing directories to each worker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mSyncUtils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSyncUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'SyncUtils'"
     ]
    }
   ],
   "source": [
    "# First, add a Python module containing utilities for syncing directories to each worker\n",
    "import SyncUtils\n",
    "sc.addFile(SyncUtils.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ps_function(server):\n",
    "  \"\"\"\n",
    "  Function that runs on each Spark executor assigned to be a parameter server; waits for the workers\n",
    "  to finish training.\n",
    "  \"\"\"\n",
    "  server.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import time\n",
    "\n",
    "def worker_function(task_index, worker_num, num_workers, num_ps, cluster, server):\n",
    "    \"\"\"\n",
    "    Function that runs on each Spark executor assigned to be a Tensorflow worker; performs model training,\n",
    "    sending updates to the parameter server.\n",
    "    \"\"\"\n",
    "    import tensorflow.contrib.slim as slim\n",
    "    from SyncUtils import syncDirs\n",
    "    # Download training data from S3; our dataset is in a public bucket, so we don't\n",
    "    # need to pass credentials when reading data\n",
    "    download_files(worker_num, num_workers, bucket_name=BUCKET_NAME, prefix=TRAIN_PREFIX,\n",
    "                              region_name=S3_REGION, local_dir=LOCAL_DATA_DIR, use_authentication=False)\n",
    "    \n",
    "    # Determine if current worker is chief\n",
    "    is_chief = (task_index == 0)\n",
    "    # Ops are assigned to worker by default.\n",
    "    with tf.device(tf.train.replica_device_setter(                     # TF for multi-node\n",
    "        worker_device=\"/job:worker/task:%d\" % task_index,\n",
    "        cluster=cluster)):\n",
    "      # Variables and their related init/assign ops are assigned to ps.\n",
    "      with slim.arg_scope(\n",
    "          [slim.variable],\n",
    "          device=slim.VariableDeviceChooser(num_ps)):\n",
    "      \n",
    "        # Create a variable to track the number of global training steps run thus far; a global step\n",
    "        # involves processing a single batch of training data on each worker & applying gradient updates.\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "        # Get a tuple (images, labels) corresponding to a batch of training data\n",
    "        images, labels = get_batch(data_dir=LOCAL_DATA_DIR, batch_size=BATCH_SIZE, num_readers=1, num_preprocess_threads=2, file_prefix='train')\n",
    "  \n",
    "        # Construct training graph, get tensors corresponding to model loss and accuracy on a batch of data\n",
    "        loss, _ = get_loss_and_accuracy(images, labels)\n",
    "        loss_summary = tf.summary.scalar(\"train_loss\", loss)\n",
    "      \n",
    "        # Add optimizers to the graph; create a SyncReplicasOptimizer and use it to compute gradients & define the train_op\n",
    "        # Also, launch and return a Tensorflow session to use for training our model\n",
    "        sess, train_op = add_optimizers_and_launch_sess(loss, LEARNING_RATE, CHECKPOINT_DIR, task_index, num_workers, global_step, server)\n",
    "      \n",
    "        # Create SummaryWriter used for writing training event files. \n",
    "        if is_chief:\n",
    "          LOCAL_EVENT_FILE_DIR = tempfile.mkdtemp()\n",
    "          summary_writer = tf.summary.FileWriter(LOCAL_EVENT_FILE_DIR, graph=sess.graph)\n",
    "          # Sets up synchronization of worker's local TensorBoard logs to DBFS\n",
    "          syncDirs(src_dir=LOCAL_EVENT_FILE_DIR, dst_dir=DBFS_EVENT_FILE_DIR, sync_wait_secs=20)\n",
    "        tf.logging.info(\"Worker %d: Session initialization complete.\" % task_index)      \n",
    "      \n",
    "        tf.logging.info(\"Beginning model training\")\n",
    "        # Perform training; local_step = number of training batches run in the current process, \n",
    "        # global_step = number of model updates performed thus far\n",
    "        local_step = 0\n",
    "        while True:\n",
    "          _, step, batch_summary, loss_val = sess.run([train_op, global_step, loss_summary, loss])\n",
    "          local_step += 1\n",
    "          now = time.time()\n",
    "          if local_step % 500 == 0:\n",
    "            print(\"%f: Worker %d: training step %d done (global step: %d), loss: %s\" %\n",
    "                (now, task_index, local_step, step, loss_val))\n",
    "            if is_chief:\n",
    "              summary_writer.add_summary(batch_summary, local_step)   # this appends to one file\n",
    "          # Stop training after we finish the specified number of global steps\n",
    "          if step >= TRAIN_STEPS:\n",
    "            break\n",
    "\n",
    "        tf.logging.info(\"Finished model training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)        # TensorFlowOnSpark logs everything otherwise\n",
    "from tensorflowonspark import TFNode, TFCluster\n",
    "\n",
    "def train(args, ctx):\n",
    "  \"\"\"\n",
    "  Function run on each Spark executor by TFCluster.run(). \n",
    "  \n",
    "  Inspects args and ctx (passed by TensorflowOnSpark) to determine whether the current process is\n",
    "  a parameter server or worker, then runs the appropriate model training logic.\n",
    "  \"\"\"\n",
    "  # Get attributes for the current training run; determine the task index of the current TF worker\n",
    "  task_index = ctx.task_index\n",
    "  num_workers = len(ctx.cluster_spec['worker'])\n",
    "  num_ps = len(ctx.cluster_spec['ps'])  \n",
    "  # TODO: This is currently based on the TFCluster.run() implementation assigning IDs to PS before workers,\n",
    "  # find a way to be agnostic to the TFCluster.run() impl.    \n",
    "  worker_num = ctx.worker_num - num_ps\n",
    "  job_name = ctx.job_name\n",
    "  \n",
    "  tf.logging.info(\"Found %s workers, %s param servers\"%(num_workers, num_ps))\n",
    "  tf.logging.info(\"Running main_fun with params worker_num = %s, job_name = %s, task_index = %s\"%(worker_num, job_name, task_index))  \n",
    "  \n",
    "  # Start cluster server\n",
    "  cluster, server = TFNode.start_cluster_server(ctx, num_gpus=1)\n",
    "\n",
    "  # Check job name, run either worker or parameter server code as appropriate\n",
    "  if job_name == \"ps\":\n",
    "    ps_function(server)    \n",
    "  else:  \n",
    "    worker_function(task_index, worker_num, num_workers, num_ps, cluster, server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0e74a990e9b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# tf_args get passed as the \"args\" argument to train()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFCluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_fun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_executors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EXECUTORS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_ps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_PS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "# tf_args get passed as the \"args\" argument to train()\n",
    "cluster = TFCluster.run(sc, map_fun=train, tf_args=[], num_executors=NUM_EXECUTORS, num_ps=NUM_PS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cluster' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-807e16b1fee5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This command blocks until training finishes, then shuts down the workers & parameter server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# NOTE: If training fails with an error, this command is not guaranteed to terminate the TensorFlow workers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cluster' is not defined"
     ]
    }
   ],
   "source": [
    "# This command blocks until training finishes, then shuts down the workers & parameter server\n",
    "# NOTE: If training fails with an error, this command is not guaranteed to terminate the TensorFlow workers.\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
